"""
train_model.py â€” Malaria Detection CNN Training Script
=======================================================
Trains a binary CNN classifier on the Parasitized vs Uninfected malaria
cell-image dataset and saves the result as 'malaria_model.h5'.

Dataset expected structure:
    data/
     â””â”€ cell_images/
           â”œâ”€ Parasitized/
           â””â”€ Uninfected/

Run:
    python train_model.py
"""

import os
import json
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR       = os.path.dirname(os.path.abspath(__file__))
DATA_DIR       = os.path.join(BASE_DIR, "data", "cell_images")   # root of Parasitized/ & Uninfected/
MODEL_PATH     = os.path.join(BASE_DIR, "malaria_model.h5")      # where the trained model is saved
HISTORY_PATH   = os.path.join(BASE_DIR, "results", "history_malaria_model.json")

IMG_SIZE       = (64, 64)      # resize every image to 64Ã—64 pixels
BATCH_SIZE     = 32            # images per training step
EPOCHS         = 15            # max training epochs (EarlyStopping may stop sooner)
VALIDATION_SPLIT = 0.20        # 20 % of data held out for validation
SEED           = 42            # reproducibility

os.makedirs(os.path.join(BASE_DIR, "results"), exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Load & preprocess the dataset
#    keras.utils.image_dataset_from_directory automatically:
#      â€¢ reads sub-folder names as class labels
#      â€¢ resizes images
#      â€¢ splits into train / validation
#    We then rescale pixel values from [0, 255] â†’ [0, 1] via
#    a Rescaling layer inside the model (avoids data leakage).
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ“‚  Loading images from: {DATA_DIR}")

train_ds = keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels       = "inferred",          # use sub-folder names
    label_mode   = "binary",            # 0 or 1 for binary classification
    image_size   = IMG_SIZE,            # resize to 64Ã—64
    batch_size   = BATCH_SIZE,
    validation_split = VALIDATION_SPLIT,
    subset       = "training",
    seed         = SEED,
    shuffle      = True,
)

val_ds = keras.utils.image_dataset_from_directory(
    DATA_DIR,
    labels       = "inferred",
    label_mode   = "binary",
    image_size   = IMG_SIZE,
    batch_size   = BATCH_SIZE,
    validation_split = VALIDATION_SPLIT,
    subset       = "validation",
    seed         = SEED,
    shuffle      = False,
)

class_names = train_ds.class_names
print(f"âœ…  Classes found: {class_names}")
print(f"    Training batches  : {len(train_ds)}")
print(f"    Validation batches: {len(val_ds)}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Performance optimisation â€” prefetch data while GPU trains
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000, seed=SEED).prefetch(AUTOTUNE)
val_ds   = val_ds.cache().prefetch(AUTOTUNE)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Build the CNN model
#    Architecture:
#      Rescaling â†’ Conv+Pool (Ã—3) â†’ Flatten â†’ Dense â†’ Dropout â†’ Output
#    â€¢ Rescaling(1/255) normalises pixels inside the model graph
#    â€¢ Three Conv blocks progressively extract features at
#      increasing abstraction (32 â†’ 64 â†’ 128 filters)
#    â€¢ Dropout(0.4) reduces overfitting
#    â€¢ Final Dense(1, sigmoid) outputs P(Parasitized)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”§  Building CNN modelâ€¦")

model = keras.Sequential([
    # -- Normalisation (kept inside model so saved model is self-contained)
    layers.Rescaling(1.0 / 255, input_shape=(*IMG_SIZE, 3)),

    # -- Data augmentation (applied only during training)
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),

    # -- Convolutional block 1
    layers.Conv2D(32, (3, 3), activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2, 2),

    # -- Convolutional block 2
    layers.Conv2D(64, (3, 3), activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2, 2),

    # -- Convolutional block 3
    layers.Conv2D(128, (3, 3), activation="relu", padding="same"),
    layers.BatchNormalization(),
    layers.MaxPooling2D(2, 2),

    # -- Classifier head
    layers.Flatten(),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.4),
    layers.Dense(1, activation="sigmoid"),   # binary â†’ sigmoid output
], name="malaria_cnn")

model.summary()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Compile the model
#    â€¢ Adam optimiser with standard learning rate
#    â€¢ binary_crossentropy loss (standard for 2-class sigmoid output)
#    â€¢ accuracy metric
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.compile(
    optimizer = keras.optimizers.Adam(learning_rate=1e-3),
    loss      = "binary_crossentropy",
    metrics   = ["accuracy"],
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Callbacks â€” automatic learning-rate reduction and early stop
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
callbacks = [
    # Save the best checkpoint (by val_accuracy)
    keras.callbacks.ModelCheckpoint(
        MODEL_PATH,
        monitor        = "val_accuracy",
        save_best_only = True,
        verbose        = 1,
    ),
    # Halve LR when val_loss plateaus for 2 epochs
    keras.callbacks.ReduceLROnPlateau(
        monitor  = "val_loss",
        factor   = 0.5,
        patience = 2,
        min_lr   = 1e-6,
        verbose  = 1,
    ),
    # Stop early if val_loss does not improve for 5 epochs
    keras.callbacks.EarlyStopping(
        monitor              = "val_loss",
        patience             = 5,
        restore_best_weights = True,
        verbose              = 1,
    ),
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Train the model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸš€  Training for up to {EPOCHS} epochsâ€¦")
history = model.fit(
    train_ds,
    validation_data = val_ds,
    epochs          = EPOCHS,
    callbacks       = callbacks,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. Evaluate on validation set
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“Š  Evaluating on validation setâ€¦")
val_loss, val_acc = model.evaluate(val_ds, verbose=1)
print(f"\n    Validation Loss    : {val_loss:.4f}")
print(f"    Validation Accuracy: {val_acc * 100:.2f}%")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9. Save training history as JSON for later analysis / plotting
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(HISTORY_PATH, "w") as f:
    json.dump(
        {k: [float(x) for x in v] for k, v in history.history.items()},
        f,
        indent=2,
    )
print(f"\nğŸ’¾  Model  saved â†’ {MODEL_PATH}")
print(f"ğŸ“  History saved â†’ {HISTORY_PATH}")
print("\nâœ…  Done!")
